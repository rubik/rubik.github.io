<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sklearn on Signal to Noise</title>
    <link>http://signal-to-noise.xyz/tags/sklearn/</link>
    <description>Recent content in Sklearn on Signal to Noise</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Created by Michele Lacchia, built with Hugo</copyright>
    <lastBuildDate>Tue, 01 Nov 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://signal-to-noise.xyz/tags/sklearn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Why you should use scikit-learn&#39;s Pipeline object</title>
      <link>http://signal-to-noise.xyz/post/sklearn-pipeline/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>http://signal-to-noise.xyz/post/sklearn-pipeline/</guid>
      <description>Copyright: visualgo  Machine learning models learn from data. It is crucial, however, that the data you feed them is specifically preprocessed and refined for the problem you want to solve. This includes data cleaning, preprocessing, feature engineering, and so on.
Very often, when presented with a dataset, I would fire up a Jupyter notebook and start exploring it interactively. The notebook is great for that task, but after a while I ended up with code that is a total mess in the global namespace.</description>
    </item>
    
    <item>
      <title>Exploration of Texas death row data</title>
      <link>http://signal-to-noise.xyz/post/texasdeathrow/</link>
      <pubDate>Sat, 29 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>http://signal-to-noise.xyz/post/texasdeathrow/</guid>
      <description>On the site Texas Department of Criminal Justice there&#39;s a page which lists all the people that have been executed since 1982, when the death penalty was reinstated, along with their last statement. The data is here. In this project we are going to explore the data and see if we can apply topic modeling to the statements.
Setup We are going to use the following packages:
 scrapy to scrape the data numpy and pandas for the data manipulation altair to create plots, and occasionally matplotlib gmaps to create a heat map over a Google Map textblob for sentiment analysis scikit-learn to do topic modeling gensim for additional topic modeling  import numpy as np import pandas as pd import matplotlib.</description>
    </item>
    
  </channel>
</rss>